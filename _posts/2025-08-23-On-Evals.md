---
layout: post
title: On LLM Evals
---

LLMs hallucinate.Your job is to ensure they don’t embarrass you, your company, or your brand.

# Table of contents
1. [Introduction](#introduction)
2. [Part 1 - Choosing the right LLM](#part1)
3. [Part 2 Human Evaluation – HHH Framework](#part2)
4. [Part 3 Scaling Evaluation – LLM as Judges](#part3)

## Introduction <a name="introduction"></a>
LLMs are powerful but unpredictable. Unlike traditional software, they don’t always give the same answer to the same input. Without structured evaluation, you’re flying blind — you can’t guarantee quality, safety, or trust. Evals are how you make sure your AI is reliable enough to put in front of customers.
Enter LLLm Evals . 
- AI evals are like unit tests for agents.
- Difference between software testing vs AI evals:
  - Software testing & unit tests are deterministic. LLM agents are non-deterministic, with multiple possible paths.
  - Integration tests rely on code/docs, but improving agents relies on data.

## Part 1 - Choosing the right LLM <a name="part1"></a>
- Start with **requirements** along these dimensions
  - **Accuracy**: example >90% ideal in legal contexts.
  - **Latency**: Medium–low is acceptable for offline jobs.
  - **Cost**: Less sensitive initially since manual legal review is expensive.
  - **Context length**: for example Must handle long docs (≈20K–1M tokens).
  - **Throughput (QPM)**: Size for expected query volume.
  - **Grounding**: Model should cite source contract text.
- Some **benchmarks**you could use , typically published by models -
  - Language understanding
  - Q&A
  - Document classification
  - Reasoning (planning, chain-of-thought)
  - Tool usage (email, APIs, CRM)

- **Model Selection** 
Use published evals/model cards (e.g., a “model matrix”) to compare options.
Example: O3 High mini may offer a strong balance of accuracy, latency, and cost.

## Part 2 Human Evaluation – HHH Framework <a name="part2"></a>
- **Helpful** → Solves problem, complete, succinct.
- **Honest** → Factual, validated, clickable links.
- **Harmless** → Ethical, legal, policy-aligned, guardrails applied.
- How to use:
    - Define north star metrics: Job completion, customer satisfaction.
    - Create yes/no evaluation questions.
    - Human evaluators score against ground truth.
    - Define launch thresholds (e.g., 60% helpfulness).
    - Update criteria with customer feedback.
      
 Start with this simple framework
  
 
## Part 3 Scaling Evaluation – LLM as Judges<a name="part3"></a>

**Challenge**: Human eval doesn’t scale.
**Solutions**:
    - Cloud APIs for moderation (toxicity, hate speech, frustration).
    - Train an LLM judge using good conversation examples.
    - Use SDKs or pre-built judges to measure: correctness, relevance, guideline adherence, context sufficiency, RAG chunk relevance.
**Process**:
    - Collect requests + responses + ground truth → apply judges → score → dashboard metrics.
    - Continuous improvement: Judges improve over time but still an evolving field.

